{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import data as gdata\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import init\n",
    "from mxnet.gluon import loss as gloss\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gdata.ArrayDataset(features, labels)\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.27938566 -0.09736729]\n",
      " [-1.8847244   2.6387691 ]\n",
      " [ 0.78679943 -0.31629395]\n",
      " [-0.21165672 -1.9151295 ]\n",
      " [ 1.0041832   0.19835497]\n",
      " [ 1.1747298   1.1240152 ]\n",
      " [-1.087749   -0.6615745 ]\n",
      " [-1.1049094   0.44587043]\n",
      " [-0.3480856   0.37118176]\n",
      " [ 1.934676   -0.28565592]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[ 5.09013   -8.530067   6.840453  10.279892   5.5313396  2.720426\n",
      "  4.265578   0.4968183  2.236942   9.036529 ]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.035193\n",
      "epoch 2, loss 0.000128\n",
      "epoch 3, loss 0.000049\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch %d, loss %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4], \n",
       " [[ 1.9997255 -3.3992538]]\n",
       " <NDArray 1x2 @cpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "true_w, dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, \n",
       " [4.199734]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, dense.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Parameter in module mxnet.gluon.parameter object:\n",
      "\n",
      "class Parameter(builtins.object)\n",
      " |  Parameter(name, grad_req='write', shape=None, dtype=<class 'numpy.float32'>, lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False, differentiable=True, stype='default', grad_stype='default')\n",
      " |  \n",
      " |  A Container holding parameters (weights) of Blocks.\n",
      " |  \n",
      " |  :py:class:`Parameter` holds a copy of the parameter on each :py:class:`Context` after\n",
      " |  it is initialized with ``Parameter.initialize(...)``. If :py:attr:`grad_req` is\n",
      " |  not ``'null'``, it will also hold a gradient array on each :py:class:`Context`::\n",
      " |  \n",
      " |      ctx = mx.gpu(0)\n",
      " |      x = mx.nd.zeros((16, 100), ctx=ctx)\n",
      " |      w = mx.gluon.Parameter('fc_weight', shape=(64, 100), init=mx.init.Xavier())\n",
      " |      b = mx.gluon.Parameter('fc_bias', shape=(64,), init=mx.init.Zero())\n",
      " |      w.initialize(ctx=ctx)\n",
      " |      b.initialize(ctx=ctx)\n",
      " |      out = mx.nd.FullyConnected(x, w.data(ctx), b.data(ctx), num_hidden=64)\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  name : str\n",
      " |      Name of this parameter.\n",
      " |  grad_req : {'write', 'add', 'null'}, default 'write'\n",
      " |      Specifies how to update gradient to grad arrays.\n",
      " |  \n",
      " |      - ``'write'`` means everytime gradient is written to grad :py:class:`NDArray`.\n",
      " |      - ``'add'`` means everytime gradient is added to the grad :py:class:`NDArray`. You need\n",
      " |        to manually call ``zero_grad()`` to clear the gradient buffer before each\n",
      " |        iteration when using this option.\n",
      " |      - 'null' means gradient is not requested for this parameter. gradient arrays\n",
      " |        will not be allocated.\n",
      " |  shape : int or tuple of int, default None\n",
      " |      Shape of this parameter. By default shape is not specified. Parameter with\n",
      " |      unknown shape can be used for :py:class:`Symbol` API, but ``init`` will throw an error\n",
      " |      when using :py:class:`NDArray` API.\n",
      " |  dtype : numpy.dtype or str, default 'float32'\n",
      " |      Data type of this parameter. For example, ``numpy.float32`` or ``'float32'``.\n",
      " |  lr_mult : float, default 1.0\n",
      " |      Learning rate multiplier. Learning rate will be multiplied by lr_mult\n",
      " |      when updating this parameter with optimizer.\n",
      " |  wd_mult : float, default 1.0\n",
      " |      Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.\n",
      " |  init : Initializer, default None\n",
      " |      Initializer of this parameter. Will use the global initializer by default.\n",
      " |  stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.\n",
      " |      The storage type of the parameter.\n",
      " |  grad_stype: {'default', 'row_sparse', 'csr'}, defaults to 'default'.\n",
      " |      The storage type of the parameter's gradient.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  grad_req : {'write', 'add', 'null'}\n",
      " |      This can be set before or after initialization. Setting ``grad_req`` to ``'null'``\n",
      " |      with ``x.grad_req = 'null'`` saves memory and computation when you don't\n",
      " |      need gradient w.r.t x.\n",
      " |  lr_mult : float\n",
      " |      Local learning rate multiplier for this Parameter. The actual learning rate\n",
      " |      is calculated with ``learning_rate * lr_mult``. You can set it with\n",
      " |      ``param.lr_mult = 2.0``\n",
      " |  wd_mult : float\n",
      " |      Local weight decay multiplier for this Parameter.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, name, grad_req='write', shape=None, dtype=<class 'numpy.float32'>, lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False, differentiable=True, stype='default', grad_stype='default')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast data and gradient of this Parameter to a new data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  data(self, ctx=None)\n",
      " |      Returns a copy of this parameter on one context. Must have been\n",
      " |      initialized on this context before. For sparse parameters, use\n",
      " |      :py:meth:`Parameter.row_sparse_data` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on ctx\n",
      " |  \n",
      " |  grad(self, ctx=None)\n",
      " |      Returns a gradient buffer for this parameter on one context.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context\n",
      " |          Desired context.\n",
      " |  \n",
      " |  initialize(self, init=None, ctx=None, default_init=<mxnet.initializer.Uniform object at 0x0000018F38B27F08>, force_reinit=False)\n",
      " |      Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.\n",
      " |      ctx : Context or list of Context, defaults to :py:meth:`context.current_context()`.\n",
      " |          Initialize Parameter on given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |      \n",
      " |          .. note::\n",
      " |              Copies are independent arrays. User is responsible for keeping\n",
      " |              their values consistent when updating.\n",
      " |              Normally :py:class:`gluon.Trainer` does this for you.\n",
      " |      \n",
      " |      default_init : Initializer\n",
      " |          Default initializer is used when both :py:func:`init`\n",
      " |          and :py:meth:`Parameter.init` are ``None``.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> weight = mx.gluon.Parameter('weight', shape=(2, 2))\n",
      " |      >>> weight.initialize(ctx=mx.cpu(0))\n",
      " |      >>> weight.data()\n",
      " |      [[-0.01068833  0.01729892]\n",
      " |       [ 0.02042518 -0.01618656]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.grad()\n",
      " |      [[ 0.  0.]\n",
      " |       [ 0.  0.]]\n",
      " |      <NDArray 2x2 @cpu(0)>\n",
      " |      >>> weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)])\n",
      " |      >>> weight.data(mx.gpu(0))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(0)>\n",
      " |      >>> weight.data(mx.gpu(1))\n",
      " |      [[-0.00873779 -0.02834515]\n",
      " |       [ 0.05484822 -0.06206018]]\n",
      " |      <NDArray 2x2 @gpu(1)>\n",
      " |  \n",
      " |  list_ctx(self)\n",
      " |      Returns a list of contexts this parameter is initialized on.\n",
      " |  \n",
      " |  list_data(self)\n",
      " |      Returns copies of this parameter on all contexts, in the same order\n",
      " |      as creation. For sparse parameters, use :py:meth:`Parameter.list_row_sparse_data`\n",
      " |      instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of NDArrays\n",
      " |  \n",
      " |  list_grad(self)\n",
      " |      Returns gradient buffers on all contexts, in the same order\n",
      " |      as :py:meth:`values`.\n",
      " |  \n",
      " |  list_row_sparse_data(self, row_id)\n",
      " |      Returns copies of the 'row_sparse' parameter on all contexts, in the same order\n",
      " |      as creation. The copy only retains rows whose ids occur in provided row ids.\n",
      " |      The parameter must have been initialized before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      row_id: NDArray\n",
      " |          Row ids to retain for the 'row_sparse' parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of NDArrays\n",
      " |  \n",
      " |  reset_ctx(self, ctx)\n",
      " |      Re-assign Parameter to other contexts.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ctx : Context or list of Context, default ``context.current_context()``.\n",
      " |          Assign Parameter to given context. If ctx is a list of Context, a\n",
      " |          copy will be made for each context.\n",
      " |  \n",
      " |  row_sparse_data(self, row_id)\n",
      " |      Returns a copy of the 'row_sparse' parameter on the same context as row_id's.\n",
      " |      The copy only retains rows whose ids occur in provided row ids.\n",
      " |      The parameter must have been initialized on this context before.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      row_id: NDArray\n",
      " |          Row ids to retain for the 'row_sparse' parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      NDArray on row_id's context\n",
      " |  \n",
      " |  set_data(self, data)\n",
      " |      Sets this parameter's value on all contexts.\n",
      " |  \n",
      " |  var(self)\n",
      " |      Returns a symbol representing this parameter.\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradient buffer on all contexts to 0. No action is taken if\n",
      " |      parameter is uninitialized or doesn't require gradient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dtype\n",
      " |      The type of the parameter.\n",
      " |      \n",
      " |      Setting the dtype value is equivalent to casting the value of the parameter\n",
      " |  \n",
      " |  grad_req\n",
      " |  \n",
      " |  shape\n",
      " |      The shape of the parameter.\n",
      " |      \n",
      " |      By default, an unknown dimension size is 0. However, when the NumPy semantic\n",
      " |      is turned on, unknown dimension size is -1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dense.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
